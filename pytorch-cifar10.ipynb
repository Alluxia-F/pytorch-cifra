{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data augmentation\n",
    "transform_train=transforms.Compose([\n",
    "    transforms.RandomCrop(32,padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.2,0.2,0.2)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.2,0.2,0.2))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "trainset=torchvision.datasets.CIFAR10(\n",
    "        root='./data',train=True,download=True,transform=transform_train)\n",
    "\n",
    "trainloader=torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the testing dataset\n",
    "testset=torchvision.datasets.CIFAR10(\n",
    "    root='./data',train=False,download=True,transform=transform_test)\n",
    "testloader=torch.utils.data.DataLoader(testset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Convulution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (gap): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "LR = 0.005\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    \"\"\"Some Information about CNN\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),  \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),  \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            nn.Dropout(p=0.05,inplace=False)\n",
    "        )\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),  \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv4 = torch.nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),  \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(p=0.05,inplace=False)\n",
    "        )\n",
    "\n",
    "        self.conv5 = torch.nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, padding=1),  \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            nn.Dropout(p=0.05,inplace=False)\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.gap = nn.AvgPool2d(4,4)\n",
    "        self.fc = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Loss Function and Optimizer\n",
    "# I choose to use a Classification Cross-Entropy loss and Adam optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(net.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tain funcion\n",
    "\n",
    "def train(model,criterion,optimizer,trainloader,epochs,log_interval=50):\n",
    "    print('-------- Training job starts ------')\n",
    "    for epoch in range(epochs):\n",
    "        running_loss=0.0\n",
    "        for step,(batch_x,batch_y) in enumerate(trainloader):\n",
    "            batch_x,batch_y=batch_x.cuda(),batch_y.cuda()\n",
    "            output=model(batch_x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss=criterion(output,batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss+=loss.item()\n",
    "            if step % log_interval == (log_interval-1):\n",
    "                print('[%d, %5d] loss: %.4f' %\n",
    "                      (epoch + 1, step + 1, running_loss / log_interval))\n",
    "                running_loss = 0.0\n",
    "    print('----- Train Finished -----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test(model, testloader):\n",
    "    print('------ Test Start -----')\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_x, test_y in testloader:\n",
    "            images, labels = test_x.cuda(), test_y.cuda()\n",
    "            output = model(images)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy of the model is: %.4f %%' % accuracy)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Training job starts ------\n",
      "[1,    50] loss: 1.6883\n",
      "[1,   100] loss: 1.3786\n",
      "[1,   150] loss: 1.2157\n",
      "[2,    50] loss: 1.0591\n",
      "[2,   100] loss: 0.9891\n",
      "[2,   150] loss: 0.9564\n",
      "[3,    50] loss: 0.8549\n",
      "[3,   100] loss: 0.8405\n",
      "[3,   150] loss: 0.8120\n",
      "[4,    50] loss: 0.7450\n",
      "[4,   100] loss: 0.7187\n",
      "[4,   150] loss: 0.7227\n",
      "[5,    50] loss: 0.6627\n",
      "[5,   100] loss: 0.6589\n",
      "[5,   150] loss: 0.6249\n",
      "[6,    50] loss: 0.6104\n",
      "[6,   100] loss: 0.6019\n",
      "[6,   150] loss: 0.5956\n",
      "[7,    50] loss: 0.5577\n",
      "[7,   100] loss: 0.5687\n",
      "[7,   150] loss: 0.5436\n",
      "[8,    50] loss: 0.5189\n",
      "[8,   100] loss: 0.5199\n",
      "[8,   150] loss: 0.5126\n",
      "[9,    50] loss: 0.5017\n",
      "[9,   100] loss: 0.4849\n",
      "[9,   150] loss: 0.4830\n",
      "[10,    50] loss: 0.4618\n",
      "[10,   100] loss: 0.4613\n",
      "[10,   150] loss: 0.4604\n",
      "[11,    50] loss: 0.4329\n",
      "[11,   100] loss: 0.4290\n",
      "[11,   150] loss: 0.4341\n",
      "[12,    50] loss: 0.4127\n",
      "[12,   100] loss: 0.4151\n",
      "[12,   150] loss: 0.4044\n",
      "[13,    50] loss: 0.4007\n",
      "[13,   100] loss: 0.3861\n",
      "[13,   150] loss: 0.3991\n",
      "[14,    50] loss: 0.3612\n",
      "[14,   100] loss: 0.3773\n",
      "[14,   150] loss: 0.3781\n",
      "[15,    50] loss: 0.3660\n",
      "[15,   100] loss: 0.3669\n",
      "[15,   150] loss: 0.3595\n",
      "[16,    50] loss: 0.3411\n",
      "[16,   100] loss: 0.3441\n",
      "[16,   150] loss: 0.3486\n",
      "[17,    50] loss: 0.3224\n",
      "[17,   100] loss: 0.3274\n",
      "[17,   150] loss: 0.3202\n",
      "[18,    50] loss: 0.3033\n",
      "[18,   100] loss: 0.3145\n",
      "[18,   150] loss: 0.3201\n",
      "[19,    50] loss: 0.3019\n",
      "[19,   100] loss: 0.3001\n",
      "[19,   150] loss: 0.3002\n",
      "[20,    50] loss: 0.2884\n",
      "[20,   100] loss: 0.2829\n",
      "[20,   150] loss: 0.3066\n",
      "[21,    50] loss: 0.2769\n",
      "[21,   100] loss: 0.2695\n",
      "[21,   150] loss: 0.2788\n",
      "[22,    50] loss: 0.2655\n",
      "[22,   100] loss: 0.2722\n",
      "[22,   150] loss: 0.2694\n",
      "[23,    50] loss: 0.2683\n",
      "[23,   100] loss: 0.2462\n",
      "[23,   150] loss: 0.2601\n",
      "[24,    50] loss: 0.2541\n",
      "[24,   100] loss: 0.2513\n",
      "[24,   150] loss: 0.2675\n",
      "[25,    50] loss: 0.2406\n",
      "[25,   100] loss: 0.2547\n",
      "[25,   150] loss: 0.2431\n",
      "----- Train Finished -----\n"
     ]
    }
   ],
   "source": [
    "train(net, criterion, optimizer, trainloader, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Test Start -----\n",
      "Accuracy of the model is: 85.0400 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85.04"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
