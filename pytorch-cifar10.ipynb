{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 75\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data augmentation\n",
    "transform_train=transforms.Compose([\n",
    "    transforms.RandomCrop(32,padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.2,0.2,0.2)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.2,0.2,0.2))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "trainset=torchvision.datasets.CIFAR10(\n",
    "        root='./data',train=True,download=True,transform=transform_train)\n",
    "\n",
    "trainloader=torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the testing dataset\n",
    "testset=torchvision.datasets.CIFAR10(\n",
    "    root='./data',train=False,download=True,transform=transform_test)\n",
    "testloader=torch.utils.data.DataLoader(testset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Convulution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (conv6): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (gap): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "LR = 0.005\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    \"\"\"Some Information about CNN\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),  \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),  \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            nn.Dropout(p=0.05,inplace=False)\n",
    "        )\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),  \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv4 = torch.nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),  \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(p=0.05,inplace=False)\n",
    "        )\n",
    "\n",
    "        self.conv5 = torch.nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, padding=1),  \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            nn.Dropout(p=0.05,inplace=False)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.conv6 = torch.nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding=1),  \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.gap = nn.AvgPool2d(4,4)\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Loss Function and Optimizer\n",
    "# I choose to use a Classification Cross-Entropy loss and Adam optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(net.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tain funcion\n",
    "\n",
    "def train(model,criterion,optimizer,trainloader,epochs,log_interval=50):\n",
    "    print('-------- Training job starts ------')\n",
    "    for epoch in range(epochs):\n",
    "        running_loss=0.0\n",
    "        for step,(batch_x,batch_y) in enumerate(trainloader):\n",
    "            batch_x,batch_y=batch_x.cuda(),batch_y.cuda()\n",
    "            output=model(batch_x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss=criterion(output,batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss+=loss.item()\n",
    "            if step % log_interval == (log_interval-1):\n",
    "                print('[%d, %5d] loss: %.4f' %\n",
    "                      (epoch + 1, step + 1, running_loss / log_interval))\n",
    "                running_loss = 0.0\n",
    "    print('----- Train Finished -----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test(model, testloader):\n",
    "    print('------ Test Start -----')\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_x, test_y in testloader:\n",
    "            images, labels = test_x.cuda(), test_y.cuda()\n",
    "            output = model(images)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy of the model is: %.4f %%' % accuracy)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Training job starts ------\n",
      "[1,    50] loss: 1.7597\n",
      "[1,   100] loss: 1.4448\n",
      "[1,   150] loss: 1.3281\n",
      "[1,   200] loss: 1.2034\n",
      "[1,   250] loss: 1.0795\n",
      "[1,   300] loss: 1.0230\n",
      "[1,   350] loss: 1.0116\n",
      "[2,    50] loss: 0.8971\n",
      "[2,   100] loss: 0.8880\n",
      "[2,   150] loss: 0.8613\n",
      "[2,   200] loss: 0.8134\n",
      "[2,   250] loss: 0.7769\n",
      "[2,   300] loss: 0.7929\n",
      "[2,   350] loss: 0.7672\n",
      "[3,    50] loss: 0.6912\n",
      "[3,   100] loss: 0.7011\n",
      "[3,   150] loss: 0.6662\n",
      "[3,   200] loss: 0.6741\n",
      "[3,   250] loss: 0.6619\n",
      "[3,   300] loss: 0.6427\n",
      "[3,   350] loss: 0.6378\n",
      "[4,    50] loss: 0.6067\n",
      "[4,   100] loss: 0.6000\n",
      "[4,   150] loss: 0.5991\n",
      "[4,   200] loss: 0.5698\n",
      "[4,   250] loss: 0.6019\n",
      "[4,   300] loss: 0.5739\n",
      "[4,   350] loss: 0.5579\n",
      "[5,    50] loss: 0.5360\n",
      "[5,   100] loss: 0.5269\n",
      "[5,   150] loss: 0.5334\n",
      "[5,   200] loss: 0.5010\n",
      "[5,   250] loss: 0.5453\n",
      "[5,   300] loss: 0.5120\n",
      "[5,   350] loss: 0.5131\n",
      "[6,    50] loss: 0.4638\n",
      "[6,   100] loss: 0.4879\n",
      "[6,   150] loss: 0.4503\n",
      "[6,   200] loss: 0.4402\n",
      "[6,   250] loss: 0.4770\n",
      "[6,   300] loss: 0.4773\n",
      "[6,   350] loss: 0.4593\n",
      "[7,    50] loss: 0.4337\n",
      "[7,   100] loss: 0.4408\n",
      "[7,   150] loss: 0.4269\n",
      "[7,   200] loss: 0.4314\n",
      "[7,   250] loss: 0.4480\n",
      "[7,   300] loss: 0.4142\n",
      "[7,   350] loss: 0.4295\n",
      "[8,    50] loss: 0.4118\n",
      "[8,   100] loss: 0.3993\n",
      "[8,   150] loss: 0.4025\n",
      "[8,   200] loss: 0.4010\n",
      "[8,   250] loss: 0.3985\n",
      "[8,   300] loss: 0.3957\n",
      "[8,   350] loss: 0.4095\n",
      "[9,    50] loss: 0.3636\n",
      "[9,   100] loss: 0.3638\n",
      "[9,   150] loss: 0.3723\n",
      "[9,   200] loss: 0.3873\n",
      "[9,   250] loss: 0.3614\n",
      "[9,   300] loss: 0.3648\n",
      "[9,   350] loss: 0.3556\n",
      "[10,    50] loss: 0.3206\n",
      "[10,   100] loss: 0.3624\n",
      "[10,   150] loss: 0.3288\n",
      "[10,   200] loss: 0.3391\n",
      "[10,   250] loss: 0.3481\n",
      "[10,   300] loss: 0.3518\n",
      "[10,   350] loss: 0.3543\n",
      "[11,    50] loss: 0.3017\n",
      "[11,   100] loss: 0.3290\n",
      "[11,   150] loss: 0.3185\n",
      "[11,   200] loss: 0.3091\n",
      "[11,   250] loss: 0.3326\n",
      "[11,   300] loss: 0.3146\n",
      "[11,   350] loss: 0.3210\n",
      "[12,    50] loss: 0.2953\n",
      "[12,   100] loss: 0.2968\n",
      "[12,   150] loss: 0.3133\n",
      "[12,   200] loss: 0.2984\n",
      "[12,   250] loss: 0.3004\n",
      "[12,   300] loss: 0.3069\n",
      "[12,   350] loss: 0.3028\n",
      "[13,    50] loss: 0.2782\n",
      "[13,   100] loss: 0.2680\n",
      "[13,   150] loss: 0.2656\n",
      "[13,   200] loss: 0.2824\n",
      "[13,   250] loss: 0.2905\n",
      "[13,   300] loss: 0.2803\n",
      "[13,   350] loss: 0.2802\n",
      "[14,    50] loss: 0.2458\n",
      "[14,   100] loss: 0.2607\n",
      "[14,   150] loss: 0.2644\n",
      "[14,   200] loss: 0.2710\n",
      "[14,   250] loss: 0.2670\n",
      "[14,   300] loss: 0.2648\n",
      "[14,   350] loss: 0.2747\n",
      "[15,    50] loss: 0.2308\n",
      "[15,   100] loss: 0.2496\n",
      "[15,   150] loss: 0.2414\n",
      "[15,   200] loss: 0.2520\n",
      "[15,   250] loss: 0.2461\n",
      "[15,   300] loss: 0.2604\n",
      "[15,   350] loss: 0.2508\n",
      "[16,    50] loss: 0.2327\n",
      "[16,   100] loss: 0.2261\n",
      "[16,   150] loss: 0.2387\n",
      "[16,   200] loss: 0.2387\n",
      "[16,   250] loss: 0.2386\n",
      "[16,   300] loss: 0.2368\n",
      "[16,   350] loss: 0.2411\n",
      "[17,    50] loss: 0.2094\n",
      "[17,   100] loss: 0.2157\n",
      "[17,   150] loss: 0.2051\n",
      "[17,   200] loss: 0.2259\n",
      "[17,   250] loss: 0.2255\n",
      "[17,   300] loss: 0.2255\n",
      "[17,   350] loss: 0.2243\n",
      "[18,    50] loss: 0.1995\n",
      "[18,   100] loss: 0.1973\n",
      "[18,   150] loss: 0.2032\n",
      "[18,   200] loss: 0.2092\n",
      "[18,   250] loss: 0.2193\n",
      "[18,   300] loss: 0.2048\n",
      "[18,   350] loss: 0.2240\n",
      "[19,    50] loss: 0.1830\n",
      "[19,   100] loss: 0.1995\n",
      "[19,   150] loss: 0.1670\n",
      "[19,   200] loss: 0.1874\n",
      "[19,   250] loss: 0.2072\n",
      "[19,   300] loss: 0.2013\n",
      "[19,   350] loss: 0.2124\n",
      "[20,    50] loss: 0.1834\n",
      "[20,   100] loss: 0.1752\n",
      "[20,   150] loss: 0.1818\n",
      "[20,   200] loss: 0.1781\n",
      "[20,   250] loss: 0.1832\n",
      "[20,   300] loss: 0.1649\n",
      "[20,   350] loss: 0.1787\n",
      "[21,    50] loss: 0.1682\n",
      "[21,   100] loss: 0.1720\n",
      "[21,   150] loss: 0.1722\n",
      "[21,   200] loss: 0.1789\n",
      "[21,   250] loss: 0.1737\n",
      "[21,   300] loss: 0.1737\n",
      "[21,   350] loss: 0.1928\n",
      "[22,    50] loss: 0.1574\n",
      "[22,   100] loss: 0.1611\n",
      "[22,   150] loss: 0.1490\n",
      "[22,   200] loss: 0.1644\n",
      "[22,   250] loss: 0.1600\n",
      "[22,   300] loss: 0.1816\n",
      "[22,   350] loss: 0.1699\n",
      "[23,    50] loss: 0.1402\n",
      "[23,   100] loss: 0.1562\n",
      "[23,   150] loss: 0.1411\n",
      "[23,   200] loss: 0.1533\n",
      "[23,   250] loss: 0.1491\n",
      "[23,   300] loss: 0.1596\n",
      "[23,   350] loss: 0.1705\n",
      "[24,    50] loss: 0.1392\n",
      "[24,   100] loss: 0.1445\n",
      "[24,   150] loss: 0.1367\n",
      "[24,   200] loss: 0.1470\n",
      "[24,   250] loss: 0.1503\n",
      "[24,   300] loss: 0.1596\n",
      "[24,   350] loss: 0.1489\n",
      "[25,    50] loss: 0.1390\n",
      "[25,   100] loss: 0.1316\n",
      "[25,   150] loss: 0.1277\n",
      "[25,   200] loss: 0.1368\n",
      "[25,   250] loss: 0.1375\n",
      "[25,   300] loss: 0.1485\n",
      "[25,   350] loss: 0.1442\n",
      "[26,    50] loss: 0.1146\n",
      "[26,   100] loss: 0.1305\n",
      "[26,   150] loss: 0.1218\n",
      "[26,   200] loss: 0.1419\n",
      "[26,   250] loss: 0.1430\n",
      "[26,   300] loss: 0.1431\n",
      "[26,   350] loss: 0.1430\n",
      "[27,    50] loss: 0.1260\n",
      "[27,   100] loss: 0.1252\n",
      "[27,   150] loss: 0.1230\n",
      "[27,   200] loss: 0.1261\n",
      "[27,   250] loss: 0.1398\n",
      "[27,   300] loss: 0.1346\n",
      "[27,   350] loss: 0.1355\n",
      "[28,    50] loss: 0.1042\n",
      "[28,   100] loss: 0.1163\n",
      "[28,   150] loss: 0.1119\n",
      "[28,   200] loss: 0.1212\n",
      "[28,   250] loss: 0.1370\n",
      "[28,   300] loss: 0.1278\n",
      "[28,   350] loss: 0.1300\n",
      "[29,    50] loss: 0.1072\n",
      "[29,   100] loss: 0.1249\n",
      "[29,   150] loss: 0.1168\n",
      "[29,   200] loss: 0.0977\n",
      "[29,   250] loss: 0.1233\n",
      "[29,   300] loss: 0.1148\n",
      "[29,   350] loss: 0.1209\n",
      "[30,    50] loss: 0.0951\n",
      "[30,   100] loss: 0.1010\n",
      "[30,   150] loss: 0.1096\n",
      "[30,   200] loss: 0.1005\n",
      "[30,   250] loss: 0.1077\n",
      "[30,   300] loss: 0.1150\n",
      "[30,   350] loss: 0.1291\n",
      "[31,    50] loss: 0.1049\n",
      "[31,   100] loss: 0.0906\n",
      "[31,   150] loss: 0.1042\n",
      "[31,   200] loss: 0.1027\n",
      "[31,   250] loss: 0.1126\n",
      "[31,   300] loss: 0.1066\n",
      "[31,   350] loss: 0.1136\n",
      "[32,    50] loss: 0.0997\n",
      "[32,   100] loss: 0.1052\n",
      "[32,   150] loss: 0.1026\n",
      "[32,   200] loss: 0.0946\n",
      "[32,   250] loss: 0.1080\n",
      "[32,   300] loss: 0.1058\n",
      "[32,   350] loss: 0.1160\n",
      "[33,    50] loss: 0.0894\n",
      "[33,   100] loss: 0.0844\n",
      "[33,   150] loss: 0.0951\n",
      "[33,   200] loss: 0.1038\n",
      "[33,   250] loss: 0.1126\n",
      "[33,   300] loss: 0.0991\n",
      "[33,   350] loss: 0.0991\n",
      "[34,    50] loss: 0.0966\n",
      "[34,   100] loss: 0.0783\n",
      "[34,   150] loss: 0.0884\n",
      "[34,   200] loss: 0.0886\n",
      "[34,   250] loss: 0.0898\n",
      "[34,   300] loss: 0.0935\n",
      "[34,   350] loss: 0.0998\n",
      "[35,    50] loss: 0.0780\n",
      "[35,   100] loss: 0.0890\n",
      "[35,   150] loss: 0.0816\n",
      "[35,   200] loss: 0.0863\n",
      "[35,   250] loss: 0.0886\n",
      "[35,   300] loss: 0.1017\n",
      "[35,   350] loss: 0.0966\n",
      "[36,    50] loss: 0.0894\n",
      "[36,   100] loss: 0.0847\n",
      "[36,   150] loss: 0.0881\n",
      "[36,   200] loss: 0.0874\n",
      "[36,   250] loss: 0.0943\n",
      "[36,   300] loss: 0.0949\n",
      "[36,   350] loss: 0.0870\n",
      "[37,    50] loss: 0.0708\n",
      "[37,   100] loss: 0.0801\n",
      "[37,   150] loss: 0.0809\n",
      "[37,   200] loss: 0.0768\n",
      "[37,   250] loss: 0.0834\n",
      "[37,   300] loss: 0.0915\n",
      "[37,   350] loss: 0.0774\n",
      "[38,    50] loss: 0.0785\n",
      "[38,   100] loss: 0.0711\n",
      "[38,   150] loss: 0.0819\n",
      "[38,   200] loss: 0.0938\n",
      "[38,   250] loss: 0.0928\n",
      "[38,   300] loss: 0.0967\n",
      "[38,   350] loss: 0.0908\n",
      "[39,    50] loss: 0.0804\n",
      "[39,   100] loss: 0.0827\n",
      "[39,   150] loss: 0.0744\n",
      "[39,   200] loss: 0.0725\n",
      "[39,   250] loss: 0.0802\n",
      "[39,   300] loss: 0.0858\n",
      "[39,   350] loss: 0.0751\n",
      "[40,    50] loss: 0.0795\n",
      "[40,   100] loss: 0.0628\n",
      "[40,   150] loss: 0.0661\n",
      "[40,   200] loss: 0.0697\n",
      "[40,   250] loss: 0.0824\n",
      "[40,   300] loss: 0.0740\n",
      "[40,   350] loss: 0.0870\n",
      "[41,    50] loss: 0.0649\n",
      "[41,   100] loss: 0.0646\n",
      "[41,   150] loss: 0.0806\n",
      "[41,   200] loss: 0.0734\n",
      "[41,   250] loss: 0.0841\n",
      "[41,   300] loss: 0.0842\n",
      "[41,   350] loss: 0.0805\n",
      "[42,    50] loss: 0.0715\n",
      "[42,   100] loss: 0.0661\n",
      "[42,   150] loss: 0.0714\n",
      "[42,   200] loss: 0.0705\n",
      "[42,   250] loss: 0.0700\n",
      "[42,   300] loss: 0.0829\n",
      "[42,   350] loss: 0.0795\n",
      "[43,    50] loss: 0.0555\n",
      "[43,   100] loss: 0.0688\n",
      "[43,   150] loss: 0.0746\n",
      "[43,   200] loss: 0.0710\n",
      "[43,   250] loss: 0.0747\n",
      "[43,   300] loss: 0.0744\n",
      "[43,   350] loss: 0.0723\n",
      "[44,    50] loss: 0.0600\n",
      "[44,   100] loss: 0.0601\n",
      "[44,   150] loss: 0.0676\n",
      "[44,   200] loss: 0.0748\n",
      "[44,   250] loss: 0.0674\n",
      "[44,   300] loss: 0.0701\n",
      "[44,   350] loss: 0.0877\n",
      "[45,    50] loss: 0.0608\n",
      "[45,   100] loss: 0.0615\n",
      "[45,   150] loss: 0.0720\n",
      "[45,   200] loss: 0.0761\n",
      "[45,   250] loss: 0.0728\n",
      "[45,   300] loss: 0.0673\n",
      "[45,   350] loss: 0.0585\n",
      "[46,    50] loss: 0.0640\n",
      "[46,   100] loss: 0.0521\n",
      "[46,   150] loss: 0.0678\n",
      "[46,   200] loss: 0.0616\n",
      "[46,   250] loss: 0.0632\n",
      "[46,   300] loss: 0.0638\n",
      "[46,   350] loss: 0.0725\n",
      "[47,    50] loss: 0.0596\n",
      "[47,   100] loss: 0.0560\n",
      "[47,   150] loss: 0.0578\n",
      "[47,   200] loss: 0.0626\n",
      "[47,   250] loss: 0.0550\n",
      "[47,   300] loss: 0.0657\n",
      "[47,   350] loss: 0.0626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48,    50] loss: 0.0569\n",
      "[48,   100] loss: 0.0596\n",
      "[48,   150] loss: 0.0580\n",
      "[48,   200] loss: 0.0662\n",
      "[48,   250] loss: 0.0711\n",
      "[48,   300] loss: 0.0671\n",
      "[48,   350] loss: 0.0665\n",
      "[49,    50] loss: 0.0552\n",
      "[49,   100] loss: 0.0630\n",
      "[49,   150] loss: 0.0548\n",
      "[49,   200] loss: 0.0624\n",
      "[49,   250] loss: 0.0582\n",
      "[49,   300] loss: 0.0625\n",
      "[49,   350] loss: 0.0628\n",
      "[50,    50] loss: 0.0608\n",
      "[50,   100] loss: 0.0584\n",
      "[50,   150] loss: 0.0524\n",
      "[50,   200] loss: 0.0588\n",
      "[50,   250] loss: 0.0594\n",
      "[50,   300] loss: 0.0565\n",
      "[50,   350] loss: 0.0707\n",
      "[51,    50] loss: 0.0470\n",
      "[51,   100] loss: 0.0567\n",
      "[51,   150] loss: 0.0569\n",
      "[51,   200] loss: 0.0583\n",
      "[51,   250] loss: 0.0613\n",
      "[51,   300] loss: 0.0627\n",
      "[51,   350] loss: 0.0649\n",
      "[52,    50] loss: 0.0547\n",
      "[52,   100] loss: 0.0533\n",
      "[52,   150] loss: 0.0516\n",
      "[52,   200] loss: 0.0546\n",
      "[52,   250] loss: 0.0608\n",
      "[52,   300] loss: 0.0651\n",
      "[52,   350] loss: 0.0531\n",
      "[53,    50] loss: 0.0530\n",
      "[53,   100] loss: 0.0586\n",
      "[53,   150] loss: 0.0527\n",
      "[53,   200] loss: 0.0519\n",
      "[53,   250] loss: 0.0564\n",
      "[53,   300] loss: 0.0574\n",
      "[53,   350] loss: 0.0558\n",
      "[54,    50] loss: 0.0559\n",
      "[54,   100] loss: 0.0643\n",
      "[54,   150] loss: 0.0584\n",
      "[54,   200] loss: 0.0538\n",
      "[54,   250] loss: 0.0527\n",
      "[54,   300] loss: 0.0491\n",
      "[54,   350] loss: 0.0552\n",
      "[55,    50] loss: 0.0498\n",
      "[55,   100] loss: 0.0491\n",
      "[55,   150] loss: 0.0546\n",
      "[55,   200] loss: 0.0538\n",
      "[55,   250] loss: 0.0588\n",
      "[55,   300] loss: 0.0547\n",
      "[55,   350] loss: 0.0580\n",
      "[56,    50] loss: 0.0507\n",
      "[56,   100] loss: 0.0483\n",
      "[56,   150] loss: 0.0488\n",
      "[56,   200] loss: 0.0452\n",
      "[56,   250] loss: 0.0525\n",
      "[56,   300] loss: 0.0538\n",
      "[56,   350] loss: 0.0630\n",
      "[57,    50] loss: 0.0462\n",
      "[57,   100] loss: 0.0447\n",
      "[57,   150] loss: 0.0479\n",
      "[57,   200] loss: 0.0548\n",
      "[57,   250] loss: 0.0516\n",
      "[57,   300] loss: 0.0622\n",
      "[57,   350] loss: 0.0523\n",
      "[58,    50] loss: 0.0544\n",
      "[58,   100] loss: 0.0494\n",
      "[58,   150] loss: 0.0519\n",
      "[58,   200] loss: 0.0510\n",
      "[58,   250] loss: 0.0517\n",
      "[58,   300] loss: 0.0534\n",
      "[58,   350] loss: 0.0440\n",
      "[59,    50] loss: 0.0461\n",
      "[59,   100] loss: 0.0527\n",
      "[59,   150] loss: 0.0516\n",
      "[59,   200] loss: 0.0474\n",
      "[59,   250] loss: 0.0487\n",
      "[59,   300] loss: 0.0485\n",
      "[59,   350] loss: 0.0501\n",
      "[60,    50] loss: 0.0462\n",
      "[60,   100] loss: 0.0439\n",
      "[60,   150] loss: 0.0488\n",
      "[60,   200] loss: 0.0540\n",
      "[60,   250] loss: 0.0478\n",
      "[60,   300] loss: 0.0484\n",
      "[60,   350] loss: 0.0510\n",
      "[61,    50] loss: 0.0509\n",
      "[61,   100] loss: 0.0424\n",
      "[61,   150] loss: 0.0362\n",
      "[61,   200] loss: 0.0404\n",
      "[61,   250] loss: 0.0512\n",
      "[61,   300] loss: 0.0477\n",
      "[61,   350] loss: 0.0494\n",
      "[62,    50] loss: 0.0452\n",
      "[62,   100] loss: 0.0443\n",
      "[62,   150] loss: 0.0479\n",
      "[62,   200] loss: 0.0413\n",
      "[62,   250] loss: 0.0391\n",
      "[62,   300] loss: 0.0406\n",
      "[62,   350] loss: 0.0442\n",
      "[63,    50] loss: 0.0467\n",
      "[63,   100] loss: 0.0325\n",
      "[63,   150] loss: 0.0419\n",
      "[63,   200] loss: 0.0475\n",
      "[63,   250] loss: 0.0567\n",
      "[63,   300] loss: 0.0576\n",
      "[63,   350] loss: 0.0475\n",
      "[64,    50] loss: 0.0484\n",
      "[64,   100] loss: 0.0452\n",
      "[64,   150] loss: 0.0438\n",
      "[64,   200] loss: 0.0567\n",
      "[64,   250] loss: 0.0438\n",
      "[64,   300] loss: 0.0506\n",
      "[64,   350] loss: 0.0494\n",
      "[65,    50] loss: 0.0442\n",
      "[65,   100] loss: 0.0384\n",
      "[65,   150] loss: 0.0413\n",
      "[65,   200] loss: 0.0456\n",
      "[65,   250] loss: 0.0440\n",
      "[65,   300] loss: 0.0440\n",
      "[65,   350] loss: 0.0461\n",
      "[66,    50] loss: 0.0400\n",
      "[66,   100] loss: 0.0406\n",
      "[66,   150] loss: 0.0380\n",
      "[66,   200] loss: 0.0494\n",
      "[66,   250] loss: 0.0464\n",
      "[66,   300] loss: 0.0515\n",
      "[66,   350] loss: 0.0510\n",
      "[67,    50] loss: 0.0445\n",
      "[67,   100] loss: 0.0432\n",
      "[67,   150] loss: 0.0444\n",
      "[67,   200] loss: 0.0541\n",
      "[67,   250] loss: 0.0488\n",
      "[67,   300] loss: 0.0416\n",
      "[67,   350] loss: 0.0463\n",
      "[68,    50] loss: 0.0370\n",
      "[68,   100] loss: 0.0412\n",
      "[68,   150] loss: 0.0400\n",
      "[68,   200] loss: 0.0382\n",
      "[68,   250] loss: 0.0374\n",
      "[68,   300] loss: 0.0443\n",
      "[68,   350] loss: 0.0462\n",
      "[69,    50] loss: 0.0379\n",
      "[69,   100] loss: 0.0398\n",
      "[69,   150] loss: 0.0442\n",
      "[69,   200] loss: 0.0499\n",
      "[69,   250] loss: 0.0436\n",
      "[69,   300] loss: 0.0406\n",
      "[69,   350] loss: 0.0432\n",
      "[70,    50] loss: 0.0387\n",
      "[70,   100] loss: 0.0388\n",
      "[70,   150] loss: 0.0399\n",
      "[70,   200] loss: 0.0426\n",
      "[70,   250] loss: 0.0504\n",
      "[70,   300] loss: 0.0528\n",
      "[70,   350] loss: 0.0438\n",
      "[71,    50] loss: 0.0291\n",
      "[71,   100] loss: 0.0351\n",
      "[71,   150] loss: 0.0312\n",
      "[71,   200] loss: 0.0324\n",
      "[71,   250] loss: 0.0365\n",
      "[71,   300] loss: 0.0401\n",
      "[71,   350] loss: 0.0456\n",
      "[72,    50] loss: 0.0415\n",
      "[72,   100] loss: 0.0385\n",
      "[72,   150] loss: 0.0372\n",
      "[72,   200] loss: 0.0432\n",
      "[72,   250] loss: 0.0382\n",
      "[72,   300] loss: 0.0394\n",
      "[72,   350] loss: 0.0417\n",
      "[73,    50] loss: 0.0376\n",
      "[73,   100] loss: 0.0429\n",
      "[73,   150] loss: 0.0404\n",
      "[73,   200] loss: 0.0343\n",
      "[73,   250] loss: 0.0363\n",
      "[73,   300] loss: 0.0391\n",
      "[73,   350] loss: 0.0418\n",
      "[74,    50] loss: 0.0420\n",
      "[74,   100] loss: 0.0422\n",
      "[74,   150] loss: 0.0377\n",
      "[74,   200] loss: 0.0355\n",
      "[74,   250] loss: 0.0410\n",
      "[74,   300] loss: 0.0415\n",
      "[74,   350] loss: 0.0396\n",
      "[75,    50] loss: 0.0378\n",
      "[75,   100] loss: 0.0403\n",
      "[75,   150] loss: 0.0333\n",
      "[75,   200] loss: 0.0385\n",
      "[75,   250] loss: 0.0387\n",
      "[75,   300] loss: 0.0380\n",
      "[75,   350] loss: 0.0397\n",
      "----- Train Finished -----\n"
     ]
    }
   ],
   "source": [
    "train(net, criterion, optimizer, trainloader, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Test Start -----\n",
      "Accuracy of the model is: 87.9000 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
